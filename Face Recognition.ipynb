{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f810e3de",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "### 1) Import libraries\n",
    "### 2) Import video\n",
    "### 3) Convert to b/w\n",
    "### 4) Extract frames\n",
    "### 5) Mark the region of face\n",
    "### 6) Create Custom Dataset for each person\n",
    "### 7) Create Train and test loaders\n",
    "### 8) Create CNN Algorithm\n",
    "### 9) Test and improve accuracy\n",
    "### 9) Identify real time image\n",
    "\n",
    "After we create the b/w images, then we will pass the images through an algorithm which will give the position of face in the image, then we will crop(resize) the image to that shape and we will fix a certain shape so as to pad these images and make every image of the same size. These images will be the input to Siamese Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fa71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_name = OrderedDict()\n",
    "person_name['Venkatesh'] = 1\n",
    "person_name['Vivek'] = 2\n",
    "person_name['Prince'] = 3\n",
    "person_name['Abhay'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252b5ff",
   "metadata": {},
   "source": [
    "## Importing video and extracting frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4953261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = cv2.VideoCapture('Sample.mp4')\n",
    "# fps = video.get(cv2.CAP_PROP_FPS)\n",
    "# print(fps)\n",
    "\n",
    "# minutes = 0\n",
    "# seconds = 5\n",
    "# total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "# # seconds = total_frames//fps\n",
    "# # minutes = seconds//60\n",
    "# # seconds = seconds % 60\n",
    "# # We can use os.chdir(path) to change the working directory\n",
    "# arr = np.array([])\n",
    "# width, height =  video.get(cv2.CAP_PROP_FRAME_WIDTH), video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "# df = pd.DataFrame(columns = np.arange(int(width)*int(height)))\n",
    "# for i in range(0,int(total_frames),10):\n",
    "# #     frame_id = int(fps*(minutes*60 + seconds))\n",
    "#     frame_id = i\n",
    "# #     print('frame id =',frame_id)\n",
    "#     video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "#     ret, frame = video.read()\n",
    "# #     gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     gray_image = frame\n",
    "# #     print(type(frame))\n",
    "# #     cv2.imshow('frame', frame); cv2.waitKey(0)\n",
    "# #     cv2.imwrite(f'{frame_id}.png', frame)\n",
    "#     img = np.asarray(gray_image)\n",
    "# #     imgs = pd.DataFrame(img.reshape(1,-1))\n",
    "# #     pd.concat([df,imgs])\n",
    "# #     df.loc[i] = imgs\n",
    "#     path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Venkatesh'\n",
    "#     cv2.imwrite(os.path.join(path , f'{frame_id}.jpg'), gray_image)\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2eb18f",
   "metadata": {},
   "source": [
    "## Crop the image till its face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fa89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# imagePath = 'sample.jpg'\n",
    "# img = cv2.imread(imagePath) # This is already a numpy array\n",
    "# gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# face_classifier = cv2.CascadeClassifier(\n",
    "#     cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "# face = face_classifier.detectMultiScale(\n",
    "#     gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40))\n",
    "# for (x, y, w, h) in face:\n",
    "#     cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "# im_crop = gray_image[y:y+h, x:x+w] # Crop the image so that only face remains\n",
    "# # im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2RGB)\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.imshow(im_crop)\n",
    "# plt.axis('off')\n",
    "# im_crop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1013c",
   "metadata": {},
   "source": [
    "## Combining above two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e1486e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract images from video, and save just the faces from all the images in a seperate folder\n",
    "def video_to_faces(video_path, label):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    # We can use os.chdir(path) to change the working directory\n",
    "#     arr = np.array([])\n",
    "    width, height =  video.get(cv2.CAP_PROP_FRAME_WIDTH), video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    df = pd.DataFrame(columns = np.arange(int(width)*int(height)))\n",
    "    face_classifier = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    max_h, max_w = 0,0\n",
    "#     path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "    path = video_path.replace('Videos', 'Images')\n",
    "    path = path.replace('.mp4', '')\n",
    "#     print(path)\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure the directory exists or create it\n",
    "    for i in range(0,int(total_frames),30):\n",
    "        frame_id = i\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        ret, frame = video.read()\n",
    "        gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face = face_classifier.detectMultiScale(\n",
    "            gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40))\n",
    "        x, y, w, h = 0,0,0,0\n",
    "#         print(face)\n",
    "        if len(face) == 0: ## If no face is detected then it will skip this frame\n",
    "            continue\n",
    "        for (x, y, w, h) in face:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "            x, y, w, h = x, y, w, h\n",
    "        im_crop = gray_image[y:y+h, x:x+w]\n",
    "#         im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2RGB)\n",
    "        im_crop = cv2.cvtColor(im_crop, cv2.COLOR_GRAY2RGB)\n",
    "        im_h, im_w, _ = im_crop.shape\n",
    "        max_h, max_w = max(im_h, max_h), max(im_w, max_w) # Find the max shape of image\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         plt.imshow(im_crop)\n",
    "#         plt.axis('off')\n",
    "#         path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "        file_path = os.path.join(path , f'{frame_id}.jpg')\n",
    "        if os.path.exists(file_path) :\n",
    "            continue\n",
    "        else:\n",
    "            cv2.imwrite(file_path, im_crop)\n",
    "    print(\"done\")\n",
    "    return max_h, max_w \n",
    "# max_h, max_w = video_to_faces('Abhay Singh.mp4', \"Abhay singh\")\n",
    "# max_h, max_w = video_to_faces('C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\\\\Abhay Singh.mp4', \"Abhay Singh\")\n",
    "# print(max_h, max_w)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692794ae",
   "metadata": {},
   "source": [
    "## Code to convert all the videos to face images and store them in seperate folders according to their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad02862d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working\n",
      "done\n",
      "Working\n",
      "done\n",
      "Working\n",
      "done\n",
      "Working\n",
      "done\n",
      "Working\n",
      "done\n",
      "Working\n",
      "done\n",
      "Working\n",
      "done\n",
      "Working\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     label \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     os.mkdir(label)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     max_h, max_w \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_to_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     print(max_h, max_w)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     max_list\u001b[38;5;241m.\u001b[39mappend((max_h,max_w))\n",
      "Cell \u001b[1;32mIn[59], line 21\u001b[0m, in \u001b[0;36mvideo_to_faces\u001b[1;34m(video_path, label)\u001b[0m\n\u001b[0;32m     19\u001b[0m video\u001b[38;5;241m.\u001b[39mset(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_POS_FRAMES, frame_id)\n\u001b[0;32m     20\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 21\u001b[0m gray_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m face \u001b[38;5;241m=\u001b[39m face_classifier\u001b[38;5;241m.\u001b[39mdetectMultiScale(\n\u001b[0;32m     23\u001b[0m     gray_image, scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m, minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, minSize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m40\u001b[39m))\n\u001b[0;32m     24\u001b[0m x, y, w, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "video_dir = \"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\"\n",
    "## Code to be used later to get a list of sub-folders in a folder\n",
    "## folders = [x[0] for x in os.walk(video_dir)]\n",
    "videos_list = [f for f in os.listdir(video_dir) if f.endswith(('.mp4'))]\n",
    "working_dir = \"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Images\"\n",
    "max_list = []\n",
    "for video in videos_list:\n",
    "    print(\"Working\")\n",
    "    video_path = video_dir + \"\\\\\" + str(video)\n",
    "    label = video.replace('.mp4','')\n",
    "#     os.mkdir(label)\n",
    "    max_h, max_w = video_to_faces(video_path, label)\n",
    "#     print(max_h, max_w)\n",
    "    max_list.append((max_h,max_w))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90390e8",
   "metadata": {},
   "source": [
    "### Function to pad the image to a certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb0c07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552 552\n",
      "(660, 660, 3)\n",
      "-54 -54 -54 -54\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "index can't contain negative values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdhano\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mJupyter Notebooks\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFace Recognition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mVenkatesh\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m330.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 13\u001b[0m img_pad \u001b[38;5;241m=\u001b[39m \u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img_pad)\n",
      "Cell \u001b[1;32mIn[54], line 8\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(img, max_h, max_w)\u001b[0m\n\u001b[0;32m      6\u001b[0m bottom \u001b[38;5;241m=\u001b[39m max_h \u001b[38;5;241m-\u001b[39m im_h \u001b[38;5;241m-\u001b[39m top \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(top,bottom, left, right)\n\u001b[1;32m----> 8\u001b[0m img_pad \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbottom\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_pad\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraypad.py:743\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`pad_width` must be of integral type.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    742\u001b[0m \u001b[38;5;66;03m# Broadcast to shape (array.ndim, 2)\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m pad_width \u001b[38;5;241m=\u001b[39m \u001b[43m_as_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(mode):\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;66;03m# Old behavior: Use user-supplied function with np.apply_along_axis\u001b[39;00m\n\u001b[0;32m    747\u001b[0m     function \u001b[38;5;241m=\u001b[39m mode\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraypad.py:514\u001b[0m, in \u001b[0;36m_as_pairs\u001b[1;34m(x, ndim, as_index)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ((x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]),) \u001b[38;5;241m*\u001b[39m ndim\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_index \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain negative values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;66;03m# Converting the array with `tolist` seems to improve performance\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# when iterating and indexing the result (see usage in `pad`)\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbroadcast_to(x, (ndim, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mValueError\u001b[0m: index can't contain negative values"
     ]
    }
   ],
   "source": [
    "def pad(img, max_h, max_w):\n",
    "    im_h, im_w, _ = img.shape\n",
    "    left = (max_w - im_w)//2\n",
    "    right = max_w - im_w - left\n",
    "    top = (max_h - im_h)//2\n",
    "    bottom = max_h - im_h - top \n",
    "    print(top,bottom, left, right)\n",
    "    img_pad = np.pad(img, pad_width=((top,bottom), (left, right), (0, 0)))\n",
    "    return img_pad\n",
    "# print(max_h, max_w)\n",
    "# img = cv2.imread(\"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Venkatesh\\\\330.jpg\")\n",
    "# print(img.shape)\n",
    "# img_pad = pad(img, max_h, max_w)\n",
    "# plt.imshow(img)\n",
    "# plt.imshow(img_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86067a",
   "metadata": {},
   "source": [
    "## Function to store the padded images in a dataset along with their label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdbe94a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# label, dataframe, label_dict\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mint\u001b[39m(max_h)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(max_w)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_and_store\u001b[39m(image_data, label,label_dict, max_h, max_w):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the folder path where your images are located\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdhano\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mJupyter Notebooks\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFace Recognition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m label\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# label, dataframe, label_dict\n",
    "image_data = pd.DataFrame(columns = np.arange(int(max_h)*int(max_w)+1))\n",
    "def pad_and_store(image_data, label,label_dict, max_h, max_w):\n",
    "# Define the folder path where your images are located\n",
    "    folder_path = \"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\\" + label\n",
    "\n",
    "# Get a list of all image files in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg'))]\n",
    "# Loop through the image files and open them\n",
    "    for image_file in image_files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, image_file)\n",
    "        image = cv2.imread(file_path)\n",
    "        img_pad = pad(image, max_h, max_w)\n",
    "        row = list(od[label])\n",
    "        img_pad_flat = img_pad.reshape(1,-1)\n",
    "        row.append(img_pad_flat)\n",
    "        image_data.append(row)\n",
    "    return image_data\n",
    "image_data = pad_and_store(image_data, label,label_dict, max_h, max_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efabb06",
   "metadata": {},
   "source": [
    "# Code to Open Webcam, draw a rectangle around face and label it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a7e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access video from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "def detect_bounding_box(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    x, y, w, h = 0,0,0,0\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        x, y, w, h = x, y, w, h\n",
    "    return faces, x, y\n",
    "while True:\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    faces, x, y = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "    title = 'Face'\n",
    "    cv2.putText(video_frame,title , (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (30,255,30), 2)\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )\n",
    "    # display the processed frame in a window named \"My Face Detection Project\"\n",
    "    # add label to the face\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac866ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
