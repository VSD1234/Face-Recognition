{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f810e3de",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "### 1) Import libraries\n",
    "### 2) Import video\n",
    "### 3) Convert to b/w\n",
    "### 4) Extract frames\n",
    "### 5) Mark the region of face\n",
    "### 6) Create Custom Dataset for each person\n",
    "### 7) Create Train and test loaders\n",
    "### 8) Create CNN Algorithm\n",
    "### 9) Test and improve accuracy\n",
    "### 9) Identify real time image\n",
    "\n",
    "After we create the b/w images, then we will pass the images through an algorithm which will give the position of face in the image, then we will crop(resize) the image to that shape and we will fix a certain shape so as to pad these images and make every image of the same size. These images will be the input to Siamese Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05fa71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from itertools import combinations \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6888e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_name = OrderedDict()\n",
    "person_name['Venkatesh'] = 1\n",
    "person_name['Vivek'] = 2\n",
    "person_name['Prince'] = 3\n",
    "person_name['Abhay'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252b5ff",
   "metadata": {},
   "source": [
    "## Importing video and extracting frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4953261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = cv2.VideoCapture('Sample.mp4')\n",
    "# fps = video.get(cv2.CAP_PROP_FPS)\n",
    "# print(fps)\n",
    "\n",
    "# minutes = 0\n",
    "# seconds = 5\n",
    "# total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "# # seconds = total_frames//fps\n",
    "# # minutes = seconds//60\n",
    "# # seconds = seconds % 60\n",
    "# # We can use os.chdir(path) to change the working directory\n",
    "# arr = np.array([])\n",
    "# width, height =  video.get(cv2.CAP_PROP_FRAME_WIDTH), video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "# df = pd.DataFrame(columns = np.arange(int(width)*int(height)))\n",
    "# for i in range(0,int(total_frames),10):\n",
    "# #     frame_id = int(fps*(minutes*60 + seconds))\n",
    "#     frame_id = i\n",
    "# #     print('frame id =',frame_id)\n",
    "#     video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "#     ret, frame = video.read()\n",
    "# #     gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     gray_image = frame\n",
    "# #     print(type(frame))\n",
    "# #     cv2.imshow('frame', frame); cv2.waitKey(0)\n",
    "# #     cv2.imwrite(f'{frame_id}.png', frame)\n",
    "#     img = np.asarray(gray_image)\n",
    "# #     imgs = pd.DataFrame(img.reshape(1,-1))\n",
    "# #     pd.concat([df,imgs])\n",
    "# #     df.loc[i] = imgs\n",
    "#     path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Venkatesh'\n",
    "#     cv2.imwrite(os.path.join(path , f'{frame_id}.jpg'), gray_image)\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2eb18f",
   "metadata": {},
   "source": [
    "## Crop the image till its face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fa89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# imagePath = 'sample.jpg'\n",
    "# img = cv2.imread(imagePath) # This is already a numpy array\n",
    "# gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# face_classifier = cv2.CascadeClassifier(\n",
    "#     cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "# face = face_classifier.detectMultiScale(\n",
    "#     gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40))\n",
    "# for (x, y, w, h) in face:\n",
    "#     cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "# im_crop = gray_image[y:y+h, x:x+w] # Crop the image so that only face remains\n",
    "# # im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2RGB)\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.imshow(im_crop)\n",
    "# plt.axis('off')\n",
    "# im_crop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1013c",
   "metadata": {},
   "source": [
    "##  Function to extract faces from video and save them in a different folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e1486e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract images from video, and save just the faces from all the images in a seperate folder\n",
    "def video_to_faces(video_path, label):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Unable to open video file at {video_path}\")\n",
    "        return\n",
    "    total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    # We can use os.chdir(path) to change the working directory\n",
    "#     arr = np.array([])\n",
    "    width, height =  video.get(cv2.CAP_PROP_FRAME_WIDTH), video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    df = pd.DataFrame(columns = np.arange(int(width)*int(height)))\n",
    "    face_classifier = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    max_h, max_w = 0,0\n",
    "#     path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "    path = video_path.replace('Videos', 'Images')\n",
    "    path = path.replace('.mp4', '')\n",
    "#     if os.path.exists(path):\n",
    "#         continue\n",
    "#     else:\n",
    "    #     print(path)\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure the directory exists or create it\n",
    "    for i in range(0,int(total_frames),10):\n",
    "        frame_id = i\n",
    "        file_path = os.path.join(path , f'{frame_id}.jpg')\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        else:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                print(f\"Error reading frame {frame_id}\")\n",
    "                continue\n",
    "            gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            face = face_classifier.detectMultiScale(\n",
    "                gray_image, scaleFactor=1.1, minNeighbors=9, minSize=(40, 40))\n",
    "            x, y, w, h = 0,0,0,0\n",
    "    #         print(face)\n",
    "            if len(face) == 0: ## If no face is detected then it will skip this frame\n",
    "                continue\n",
    "            for (x, y, w, h) in face:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "                x, y, w, h = x, y, w, h\n",
    "            im_crop = gray_image[y:y+h, x:x+w]\n",
    "    #         im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2RGB)\n",
    "            im_crop = cv2.cvtColor(im_crop, cv2.COLOR_GRAY2RGB)\n",
    "            im_h, im_w, _ = im_crop.shape\n",
    "            max_h, max_w = max(im_h, max_h), max(im_w, max_w) # Find the max shape of image\n",
    "    #         import matplotlib.pyplot as plt\n",
    "    #         plt.figure(figsize=(10,10))\n",
    "    #         plt.imshow(im_crop)\n",
    "    #         plt.axis('off')\n",
    "    #         path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "            cv2.imwrite(file_path, im_crop)\n",
    "    print(\"done\")\n",
    "    return max_h, max_w \n",
    "# max_h, max_w = video_to_faces('Abhay Singh.mp4', \"Abhay singh\")\n",
    "# max_h, max_w = video_to_faces('C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\\\\Abhay Singh.mp4', \"Abhay Singh\")\n",
    "# print(max_h, max_w)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692794ae",
   "metadata": {},
   "source": [
    "## Code to convert all the videos to face images and store them in seperate folders according to their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad02862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shrinath Mishra.mp4:   3%|██                                                            | 1/30 [00:15<07:27, 15.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:   7%|████▏                                                         | 2/30 [02:00<31:51, 68.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  10%|██████▏                                                       | 3/30 [03:48<38:50, 86.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  13%|████████▎                                                     | 4/30 [05:26<39:21, 90.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  17%|██████████▎                                                   | 5/30 [05:48<27:34, 66.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  20%|████████████▍                                                 | 6/30 [07:17<29:34, 73.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  23%|██████████████▍                                               | 7/30 [08:43<29:48, 77.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  27%|████████████████▌                                             | 8/30 [10:18<30:33, 83.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  30%|██████████████████▌                                           | 9/30 [11:51<30:13, 86.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading frame 690\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  33%|████████████████████▎                                        | 10/30 [12:13<22:10, 66.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  37%|██████████████████████▎                                      | 11/30 [13:46<23:36, 74.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  40%|████████████████████████▍                                    | 12/30 [14:07<17:28, 58.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  43%|██████████████████████████▍                                  | 13/30 [15:38<19:17, 68.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  47%|████████████████████████████▍                                | 14/30 [15:59<14:25, 54.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  50%|██████████████████████████████▌                              | 15/30 [16:22<11:07, 44.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  53%|████████████████████████████████▌                            | 16/30 [18:14<15:07, 64.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  57%|██████████████████████████████████▌                          | 17/30 [19:53<16:17, 75.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  60%|████████████████████████████████████▌                        | 18/30 [21:28<16:12, 81.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  63%|██████████████████████████████████████▋                      | 19/30 [23:04<15:42, 85.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  67%|████████████████████████████████████████▋                    | 20/30 [23:53<12:25, 74.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading frame 740\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  70%|██████████████████████████████████████████▋                  | 21/30 [24:14<08:46, 58.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  73%|████████████████████████████████████████████▋                | 22/30 [25:46<09:07, 68.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  77%|██████████████████████████████████████████████▊              | 23/30 [26:05<06:16, 53.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  80%|████████████████████████████████████████████████▊            | 24/30 [27:45<06:45, 67.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  83%|██████████████████████████████████████████████████▊          | 25/30 [28:39<05:17, 63.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  87%|████████████████████████████████████████████████████▊        | 26/30 [30:23<05:03, 75.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  90%|██████████████████████████████████████████████████████▉      | 27/30 [30:46<02:59, 59.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  93%|████████████████████████████████████████████████████████▉    | 28/30 [31:40<01:56, 58.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Shrinath Mishra.mp4:  97%|██████████████████████████████████████████████████████████▉  | 29/30 [32:04<00:47, 47.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shrinath Mishra.mp4: 100%|█████████████████████████████████████████████████████████████| 30/30 [32:27<00:00, 64.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Just Replace this path with the path to your video directory\n",
    "video_dir = \"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\"\n",
    "## Code to be used later to get a list of sub-folders in a folder\n",
    "## folders = [x[0] for x in os.walk(video_dir)]\n",
    "videos_list = [f for f in os.listdir(video_dir) if f.endswith(('.mp4'))]\n",
    "max_list = []\n",
    "for video in tqdm(videos_list):\n",
    "#     print(\"Working on\", video) \n",
    "    video_path = video_dir + \"\\\\\" + str(video)\n",
    "    label = video.replace('.mp4','')\n",
    "#     os.mkdir(label)\n",
    "    max_h, max_w = video_to_faces(video_path, label)\n",
    "#     print(max_h, max_w)\n",
    "    max_list.append([max_h,max_w])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90390e8",
   "metadata": {},
   "source": [
    "### Function to pad the image to a certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb0c07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841 841\n"
     ]
    }
   ],
   "source": [
    "# max_list = np.array(max_list).astype(int)\n",
    "max_h, max_w = (np.max(max_list,axis = 0))\n",
    "print(max_h, max_w)\n",
    "def pad(img, max_h, max_w):\n",
    "    im_h, im_w, _ = img.shape\n",
    "    left = (max_w - im_w)//2\n",
    "    right = max_w - im_w - left\n",
    "    top = (max_h - im_h)//2\n",
    "    bottom = max_h - im_h - top \n",
    "#     print(max_h, max_w, im_h, im_w)\n",
    "    img_pad = np.pad(img, pad_width=((top,bottom), (left, right), (0, 0)))\n",
    "    return img_pad\n",
    "# print(max_h, max_w)\n",
    "# img = cv2.imread(\"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Venkatesh\\\\330.jpg\")\n",
    "# print(img.shape)\n",
    "# img_pad = pad(img, max_h, max_w)\n",
    "# plt.imshow(img)\n",
    "# plt.imshow(img_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86067a",
   "metadata": {},
   "source": [
    "## Function to store the padded images in seperate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecdbe94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:00<00:00, 424.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# label, dataframe, label_dict\n",
    "# image_data = pd.DataFrame(columns = np.arange(int(max_h)*int(max_w)+1))\n",
    "def pad_and_store(images_path, max_h, max_w):\n",
    "    '''\n",
    "    images_path = Path of the folder inside which there are folders of each person's name\n",
    "                  containing images of their faces\n",
    "    max_h, max_w = Max height and width of the images of faces, we will pad all other images\n",
    "                    to make them of this shape\n",
    "    '''\n",
    "    # List of folders inside the Images folder\n",
    "    folders = [x[0] for x in os.walk(images_path)]\n",
    "    for folder in tqdm(folders):\n",
    "        # Get a list of all image files in the folder\n",
    "        folder_path = os.path.join(images_path,folder)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg'))]\n",
    "        pad_image_dir = folder_path.replace(\"Images\", \"Images_padded\")\n",
    "        os.makedirs(pad_image_dir, exist_ok=True)\n",
    "        # Loop through the image files and open them\n",
    "        for image_file in image_files:\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, image_file)\n",
    "            save_path = os.path.join(pad_image_dir, image_file)\n",
    "            if os.path.exists(save_path):\n",
    "                continue\n",
    "            else:\n",
    "                image = cv2.imread(file_path)\n",
    "                img_pad = pad(image, max_h, max_w)\n",
    "                cv2.imwrite(save_path, img_pad)\n",
    "#             row = list(od[label])\n",
    "#             img_pad_flat = img_pad.reshape(1,-1)\n",
    "#             row.append(img_pad_flat)\n",
    "#             image_data.append(row)\n",
    "    print(\"Done\")\n",
    "    return \n",
    "image_dir = video_dir.replace(\"Videos\", \"Images\")\n",
    "image_data = pad_and_store(image_dir, max_h, max_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802aa99e",
   "metadata": {},
   "source": [
    "## Generate Random Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64128c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_triplets(images_path):\n",
    "    '''\n",
    "    We will choose the number of triplets based on the number of images in each folder.\n",
    "    Lets say we will choose at max 8 pairs of anchor and positive images from a folder.\n",
    "    If a folder has less than 5 images then we will skip it.\n",
    "    We will calculate the number of images in a folder then by using nC2 we will calculate number of \n",
    "    pairs we can choose.\n",
    "    '''\n",
    "    folders = [x[0] for x in os.walk(images_path)]\n",
    "    for folder in folders:\n",
    "        path = os.path.join(images_path)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg'))]\n",
    "        if len(image_files) < 5:\n",
    "            continue\n",
    "        \n",
    "\n",
    "image_dir = video_dir.replace(\"Videos\", \"Images\")\n",
    "generate_random_triplets(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f9ba8",
   "metadata": {},
   "source": [
    "## Triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededaece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor, positive, negative, margin=0.2):\n",
    "    pos_dist = (anchor - positive).pow(2).sum(-1) #.pow(.5)\n",
    "    neg_dist = (anchor - negative).pow(2).sum(-1) #.pow(.5)\n",
    "    loss = F.relu(pos_dist - neg_dist + margin)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efabb06",
   "metadata": {},
   "source": [
    "# Code to Open Webcam, draw a rectangle around face and label it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a7e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access video from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "def detect_bounding_box(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    x, y, w, h = 0,0,0,0\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        x, y, w, h = x, y, w, h\n",
    "    return faces, x, y\n",
    "while True:\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    faces, x, y = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "    title = 'Face'\n",
    "    cv2.putText(video_frame,title , (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (30,255,30), 2)\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )\n",
    "    # display the processed frame in a window named \"My Face Detection Project\"\n",
    "    # add label to the face\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac866ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
