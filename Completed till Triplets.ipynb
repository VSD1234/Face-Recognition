{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f810e3de",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "### 1) Import libraries\n",
    "### 2) Import video\n",
    "### 3) Convert to b/w\n",
    "### 4) Extract frames\n",
    "### 5) Mark the region of face\n",
    "### 6) Create Custom Dataset for each person\n",
    "### 7) Create Train and test loaders\n",
    "### 8) Create CNN Algorithm\n",
    "### 9) Test and improve accuracy\n",
    "### 9) Identify real time image\n",
    "\n",
    "After we create the b/w images, then we will pass the images through an algorithm which will give the position of face in the image, then we will crop(resize) the image to that shape and we will fix a certain shape so as to pad these images and make every image of the same size. These images will be the input to Siamese Network.\n",
    "\n",
    "Reference video to import files from gdrive into colab: https://www.youtube.com/watch?v=BuuH0wsJ8-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fa71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from itertools import combinations \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## Just Replace this path with the path to your video directory\n",
    "video_dir = \"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\"\n",
    "\n",
    "# if os.makedirs(path, exist_ok=True)\n",
    "# Image_Dataset = pd.DataFrame(columns = ['label'] + [f'pixel_{i}' for i in range(600*600)])\n",
    "# global Image_Dataset\n",
    "# Triplet_Dataset = pd.DataFrame()\n",
    "# Image_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6888e1aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m person_name[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVrukshal Balki\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m23\u001b[39m\n\u001b[0;32m     26\u001b[0m person_name[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYash Gupta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mOrderedDict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "person_name = OrderedDict()\n",
    "person_name[\"Abhay Singh\"] = 0\n",
    "person_name[\"Abhishek Kumar\"] = 1\n",
    "person_name[\"Ajay Kumar\"] = 2\n",
    "person_name[\"Anand\"] = 3\n",
    "person_name[\"Ankit Soni\"] = 4\n",
    "person_name[\"Arjun Prajapat\"] = 5\n",
    "person_name[\"Arpit Jain\"] = 6\n",
    "person_name[\"Kushal Barochia\"] = 7\n",
    "person_name[\"Nakul Singh\"] = 8\n",
    "person_name[\"Ojas Lunia\"] = 9\n",
    "person_name[\"Pratik Wani\"] = 10\n",
    "person_name[\"Praveen\"] = 11\n",
    "person_name[\"Prince Kumar\"] = 12\n",
    "person_name[\"Raghav\"] = 13\n",
    "person_name[\"Rugved Kalaskar\"] = 14\n",
    "person_name[\"Shantanu\"] = 15\n",
    "person_name[\"Shrinath Mishra\"] = 16\n",
    "person_name[\"Siddharth Gupta\"] = 17\n",
    "person_name[\"Sohail\"] = 18\n",
    "person_name[\"Tejas Mathurkar\"] = 19\n",
    "person_name[\"Venkatesh\"] = 20\n",
    "person_name[\"Vikash\"] = 21\n",
    "person_name[\"Vivek\"] = 22\n",
    "person_name[\"Vrukshal Balki\"] = 23\n",
    "person_name[\"Yash Gupta\"] = 24\n",
    "OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1013c",
   "metadata": {},
   "source": [
    "##  Function to extract faces from video, crop them to same size and save them in a different folder\n",
    "Try adding data by columns instead of by rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e1486e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to extract images from video, and save just the faces from all the images in a seperate folder\n",
    "def video_to_faces(video_path, label,img_size):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Unable to open video file at {video_path}\")\n",
    "        return 0,0\n",
    "    total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    # We can use os.chdir(path) to change the working directory\n",
    "#     arr = np.array([])\n",
    "    width, height =  video.get(cv2.CAP_PROP_FRAME_WIDTH), video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    df = pd.DataFrame(columns = np.arange(int(width)*int(height)))\n",
    "    face_classifier = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    max_h, max_w = 600,600\n",
    "#     path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "    path = video_path.replace('Videos', 'Images')\n",
    "    path = path.replace('.mp4', '')\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure the directory exists or create it\n",
    "    for i in range(0,int(total_frames),10):\n",
    "        frame_id = i\n",
    "        file_path = os.path.join(path , f'{frame_id}.jpg')\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        else:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                print(f\"Error reading frame {frame_id}\")\n",
    "                continue\n",
    "            gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            face = face_classifier.detectMultiScale(\n",
    "                gray_image, scaleFactor=1.2, minNeighbors=9, minSize=(250, 250), maxSize=(max_h  ,max_w))\n",
    "#             x, y, w, h = 0,0,0,0\n",
    "    #         print(face)\n",
    "            if len(face) == 0: ## If no face is detected then it will skip this frame\n",
    "                continue\n",
    "            for (x, y, w, h) in face:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "                x1 = abs(max_h//2 - x - w//2)\n",
    "                x2 = abs(max_h//2 + x + w//2)\n",
    "                y1 = abs(max_w//2 - y - h//2)\n",
    "                y2 = abs(max_w//2 + y + h//2)\n",
    "                im_crop = gray_image[y1:y2, x1:x2]\n",
    "                im_crop = cv2.resize(im_crop, (img_size, img_size), interpolation=cv2.INTER_AREA) # try using 100*100\n",
    "#                 flat_img = im_crop.flatten()\n",
    "#                 print(flat_img)\n",
    "#                 Modified the following code so that the image gets flattened and we will add the flattened image along with its label in. new rules. stop.\n",
    "#                 new_row = pd.DataFrame([[label] + flat_img.tolist()])\n",
    "#                 Image_Dataset = pd.concat([Image_Dataset, new_row], ignore_index=True)\n",
    "    #         im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2RGB)\n",
    "#             im_crop = cv2.cvtColor(im_crop, cv2.COLOR_GRAY2RGB)\n",
    "#             import matplotlib.pyplot as plt\n",
    "#             plt.figure(figsize=(10,10))\n",
    "#             plt.imshow(im_crop, cmap = 'gray')\n",
    "#             plt.axis('off')\n",
    "    #         path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "#             print(im_crop.shape)\n",
    "                cv2.imwrite(file_path, im_crop)\n",
    "#     print(\"done\")\n",
    "    return \n",
    "# video_to_faces('Abhay Singh.mp4', \"Abhay singh\")\n",
    "# video_to_faces('C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\\\\Abhay Singh.mp4', \"Abhay Singh\")\n",
    "# print(max_h, max_w)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692794ae",
   "metadata": {},
   "source": [
    "## Code to convert all the videos to face images and store them in seperate folders according to their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad02862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Code to be used later to get a list of sub-folders in a folder\n",
    "## folders = [x[0] for x in os.walk(video_dir)]\n",
    "img_size = 150\n",
    "# Image_Dataset = pd.DataFrame(columns = ['label'] + [f'pixel_{i}' for i in range(img_size*img_size)])\n",
    "videos_list = [f for f in os.listdir(video_dir) if f.endswith(('.mp4'))]\n",
    "for video in tqdm(videos_list):\n",
    "#     print(\"Working on\", video) \n",
    "    video_path = video_dir + \"\\\\\" + str(video)\n",
    "    label = video.replace('.mp4','')\n",
    "    video_to_faces(video_path, label,img_size)\n",
    "#     max_h, max_w = max(h, max_h), max(w, max_w)\n",
    "#     print(max_h, max_w)\n",
    "#     Image_Dataset.to_csv('Image_Dataset.csv')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713f42c",
   "metadata": {},
   "source": [
    "## Generate Random Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d205218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "triplets_data = pd.DataFrame() \n",
    "## add all the created triplets to a Dataframe by flattening them by using df.loc[i] and save it to an excel file\n",
    "def generate_random_triplets(images_path, pairs_per_class):\n",
    "    '''\n",
    "    We will choose the number of triplets based on the number of images in each folder.\n",
    "    Lets say we will choose at max 8 pairs of anchor and positive images from a folder.\n",
    "    If a folder has less than 5 images then we will skip it.\n",
    "    '''\n",
    "    triplets_path = images_path.replace(\"Images\", \"Triplets\")\n",
    "    os.makedirs(triplets_path, exist_ok=True)\n",
    "    anchor = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    folders = [x[0] for x in os.walk(images_path)]\n",
    "    folders = folders[1:]\n",
    "    for folder in tqdm(folders):\n",
    "#         print(folders)\n",
    "        path = os.path.join(images_path, folder)\n",
    "        image_files = [f for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "#         print(\"Image_files\", image_files)\n",
    "        if not image_files:\n",
    "            continue\n",
    "        if len(image_files) < 4:\n",
    "            #Skip folder with less than 8 images\n",
    "            continue\n",
    "        for i in range(pairs_per_class):\n",
    "            current_path = os.path.join(triplets_path, str(f'{folders.index(folder)}_{i}'))\n",
    "            os.makedirs(current_path, exist_ok=True)\n",
    "            ## Choose anchor and positive images\n",
    "            # Choose 2 random numbers to be the indexes for anchor and pos image\n",
    "            anchor_idx, positive_idx = np.random.choice(len(image_files), size=2, replace=False)\n",
    "#             print(anchor_idx, )\n",
    "            anchor_image = cv2.imread(os.path.join(path, image_files[anchor_idx]))\n",
    "#             anchor_image = Image.open(os.path.join(images_path, image_files[anchor_idx]))\n",
    "#             print(\"anchor\", image_files[anchor_idx])\n",
    "            pos_image = cv2.imread(os.path.join(path, image_files[positive_idx]))\n",
    "#             print('pos', pos_image)\n",
    "            # For neg image we will randomly choose a folder and randomly choose an image from it\n",
    "            neg_index = np.random.choice([x for x in range(0, len(folders)) if x != folder]) \n",
    "            neg_folder = os.path.join(images_path, folders[neg_index])\n",
    "            neg_image_files = [f for f in os.listdir(neg_folder) if f.endswith(('.jpg'))]\n",
    "            if not neg_image_files:\n",
    "                continue\n",
    "            neg_image = cv2.imread(os.path.join(neg_folder, np.random.choice(neg_image_files)))\n",
    "#             print('neg shape', neg_image.shape)\n",
    "#             print(folders[neg_index], len(neg_image_files))\n",
    "#             if len(neg_image_files) > 4:\n",
    "#                 neg_img = np.random.randint(0, len(neg_image_files))\n",
    "#                 path_neg = os.path.join(neg_folder, neg_image_files[neg_img])\n",
    "#                 img = cv2.imread(path_neg)\n",
    "#                 neg.append(img)\n",
    "#             fig = plt.figure(figsize=(10,7))\n",
    "#             rows, cols = 1,3\n",
    "#             fig.add_subplot(rows, cols, 1)\n",
    "#             plt.imshow(anchor_image, cmap = 'gray')\n",
    "#             plt.axis('off')\n",
    "#             plt.title('Anchor Image')\n",
    "#             fig.add_subplot(rows, cols, 1)\n",
    "#             plt.imshow(pos_image)\n",
    "#             plt.axis('off')\n",
    "#             plt.title('Positive Image')\n",
    "#             fig.add_subplot(rows, cols, 1)\n",
    "#             plt.imshow(neg_image)\n",
    "#             plt.axis('off')\n",
    "#             plt.title('Negative Image')\n",
    "#             anchor_image = cv2.imread(os.path.join(images_path, image_files[anchor_idx]))\n",
    "#             print(\"Anchor Image Shape:\", anchor_image.shape)  # Add this line to check the shape\n",
    "#             pos_image = cv2.imread(os.path.join(images_path, image_files[positive_idx]))\n",
    "#             print(\"Positive Image Shape:\", pos_image.shape)  # Add this line to check the shape\n",
    "#             neg_image = cv2.imread(os.path.join(neg_folder, np.random.choice(neg_image_files)))\n",
    "#             print(\"Negative Image Shape:\", neg_image.shape) \n",
    "            cv2.imwrite(os.path.join(current_path, \"anchor.jpg\"),anchor_image)\n",
    "            cv2.imwrite(os.path.join(current_path, \"pos.jpg\"),pos_image)\n",
    "            cv2.imwrite(os.path.join(current_path, \"neg.jpg\"),neg_image)\n",
    "            anchor.append(anchor_image)\n",
    "            pos.append(pos_image)\n",
    "            neg.append(neg_image)\n",
    "    return anchor, pos, neg\n",
    "image_dir = video_dir.replace(\"Videos\", \"Images\")\n",
    "anchor, pos, neg = generate_random_triplets(image_dir,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52db9cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 150, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m triplets \u001b[38;5;241m=\u001b[39m [pos, anchor, neg]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# triplets = np.array(triplets, dtype = object)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtriplets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame(triplets)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# df.to_excel(\"Triplets.xlsx\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# df\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# plt.imshow(neg[0])\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# print(neg.shape)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72ae2f60",
   "metadata": {},
   "source": [
    "## Triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4a7ea996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor, positive, negative, margin=0.2):\n",
    "    pos_dist = (anchor - positive).pow(2).sum(-1) #.pow(.5)\n",
    "    neg_dist = (anchor - negative).pow(2).sum(-1) #.pow(.5)\n",
    "    loss = F.relu(pos_dist - neg_dist + margin)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3d5d5",
   "metadata": {},
   "source": [
    "## Custom Dataset Class for the triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6088bf43",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[181], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg[idx]\n\u001b[1;32m---> 15\u001b[0m Triplet_Dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[181], line 4\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, triplet_pairs, transform)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, triplet_pairs, transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m----> 4\u001b[0m     anchor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mtriplet_pairs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      5\u001b[0m     pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(triplet_pairs[:,:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      6\u001b[0m     neg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(triplet_pairs[:,:,\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, triplet_folder, transform = transforms.ToTensor()):\n",
    "        self.transform = transform\n",
    "        folders = [x[0] for x in os.walk(images_path)]\n",
    "        self.folders = folders[1:]\n",
    "#         os.makedirs(current_path, exist_ok=True)\n",
    "        path = os.path.join(images_path, folder)\n",
    "        \n",
    "#         self.anchor = torch.from_numpy(anchor)\n",
    "#         self.pos = torch.from_numpy(anchor)\n",
    "#         self.neg = torch.from_numpy(neg)\n",
    "    def __len__(self):\n",
    "        return len(self.folders)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        folder = folders[idx]\n",
    "        current_path = os.path.join(triplet_folder, str(folders.index(folder)))\n",
    "        image_files = [f for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "        \n",
    "        \n",
    "        return self.anchor[idx], self.pos[idx], self.neg[idx]\n",
    "triplet_folder = video_dir.replace(\"Videos\", \"Triplets\")\n",
    "Triplet_Dataset = Dataset(triplet_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Dataset into train and test\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(Triplet_Dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177bf959",
   "metadata": {},
   "source": [
    "## CNN Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cad45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffee4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf2e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7085c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ab215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c6e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6efabb06",
   "metadata": {},
   "source": [
    "# Code to Open Webcam, draw a rectangle around face and label it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "34a7e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access video from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "def detect_bounding_box(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    x, y, w, h = 0,0,0,0\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        x, y, w, h = x, y, w, h\n",
    "    return faces, x, y\n",
    "while True:\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    faces, x, y = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "    title = 'Face'\n",
    "    cv2.putText(video_frame,title , (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (30,255,30), 2)\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )\n",
    "    # display the processed frame in a window named \"My Face Detection Project\"\n",
    "    # add label to the face\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac866ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
