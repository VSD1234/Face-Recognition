{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f810e3de",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "### 1) Import libraries\n",
    "### 2) Import video\n",
    "### 3) Convert to b/w\n",
    "### 4) Extract frames\n",
    "### 5) Mark the region of face\n",
    "### 6) Create Custom Dataset for each person\n",
    "### 7) Create Train and test loaders\n",
    "### 8) Create CNN Algorithm\n",
    "### 9) Test and improve accuracy\n",
    "### 9) Identify real time image\n",
    "\n",
    "After we create the b/w images, then we will pass the images through an algorithm which will give the position of face in the image, then we will crop(resize) the image to that shape and we will fix a certain shape so as to pad these images and make every image of the same size. These images will be the input to Siamese Network.\n",
    "\n",
    "Reference video to import files from gdrive into colab: https://www.youtube.com/watch?v=BuuH0wsJ8-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fa71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from itertools import combinations \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## Just Replace this path with the path to your video directory\n",
    "video_dir = \"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\"\n",
    "\n",
    "# if os.makedirs(path, exist_ok=True)\n",
    "# Image_Dataset = pd.DataFrame(columns = ['label'] + [f'pixel_{i}' for i in range(600*600)])\n",
    "# global Image_Dataset\n",
    "# Triplet_Dataset = pd.DataFrame()\n",
    "# Image_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6888e1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_name = OrderedDict()\n",
    "person_name[\"Abhay Singh\"] = 0\n",
    "person_name[\"Abhishek Kumar\"] = 1\n",
    "person_name[\"Ajay Kumar\"] = 2\n",
    "person_name[\"Anand\"] = 3\n",
    "person_name[\"Ankit Soni\"] = 4\n",
    "person_name[\"Arjun Prajapat\"] = 5\n",
    "person_name[\"Arpit Jain\"] = 6\n",
    "person_name[\"Kushal Barochia\"] = 7\n",
    "person_name[\"Nakul Singh\"] = 8\n",
    "person_name[\"Ojas Lunia\"] = 9\n",
    "person_name[\"Pratik Wani\"] = 10\n",
    "person_name[\"Praveen\"] = 11\n",
    "person_name[\"Prince Kumar\"] = 12\n",
    "person_name[\"Raghav\"] = 13\n",
    "person_name[\"Rugved Kalaskar\"] = 14\n",
    "person_name[\"Shantanu\"] = 15\n",
    "person_name[\"Shrinath Mishra\"] = 16\n",
    "person_name[\"Siddharth Gupta\"] = 17\n",
    "person_name[\"Sohail\"] = 18\n",
    "person_name[\"Tejas Mathurkar\"] = 19\n",
    "person_name[\"Venkatesh\"] = 20\n",
    "person_name[\"Vikash\"] = 21\n",
    "person_name[\"Vivek\"] = 22\n",
    "person_name[\"Vrukshal Balki\"] = 23\n",
    "person_name[\"Yash Gupta\"] = 24\n",
    "OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1013c",
   "metadata": {},
   "source": [
    "##  Function to extract faces from video, crop them to same size and save them in a different folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e1486e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to extract images from video, and save just the faces from all the images in a seperate folder\n",
    "def video_to_faces(video_path, label,img_size):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Unable to open video file at {video_path}\")\n",
    "        return 0,0\n",
    "    total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    # We can use os.chdir(path) to change the working directory\n",
    "#     arr = np.array([])\n",
    "    width, height =  video.get(cv2.CAP_PROP_FRAME_WIDTH), video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    df = pd.DataFrame(columns = np.arange(int(width)*int(height)))\n",
    "    face_classifier = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    max_h, max_w = 600,600\n",
    "#     path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "    path = video_path.replace('Videos', 'Images')\n",
    "    path = path.replace('.mp4', '')\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure the directory exists or create it\n",
    "    for i in range(0,int(total_frames),10):\n",
    "        frame_id = i\n",
    "        file_path = os.path.join(path , f'{frame_id}.jpg')\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        else:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                print(f\"Error reading frame {frame_id}\")\n",
    "                continue\n",
    "            gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            face = face_classifier.detectMultiScale(\n",
    "                gray_image, scaleFactor=1.2, minNeighbors=9, minSize=(250, 250), maxSize=(max_h  ,max_w))\n",
    "#             x, y, w, h = 0,0,0,0\n",
    "    #         print(face)\n",
    "            if len(face) == 0: ## If no face is detected then it will skip this frame\n",
    "                continue\n",
    "            for (x, y, w, h) in face:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "                x1 = abs(max_h//2 - x - w//2)\n",
    "                x2 = abs(max_h//2 + x + w//2)\n",
    "                y1 = abs(max_w//2 - y - h//2)\n",
    "                y2 = abs(max_w//2 + y + h//2)\n",
    "                im_crop = gray_image[y1:y2, x1:x2]\n",
    "                im_crop = cv2.resize(im_crop, (img_size, img_size), interpolation=cv2.INTER_AREA) # try using 100*100\n",
    "#                 flat_img = im_crop.flatten()\n",
    "#                 print(flat_img)\n",
    "#                 Modified the following code so that the image gets flattened and we will add the flattened image along with its label in. new rules. stop.\n",
    "#                 new_row = pd.DataFrame([[label] + flat_img.tolist()])\n",
    "#                 Image_Dataset = pd.concat([Image_Dataset, new_row], ignore_index=True)\n",
    "                im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2RGB)\n",
    "#                 im_crop = cv2.cvtColor(im_crop, cv2.COLOR_GRAY2RGB)\n",
    "#             import matplotlib.pyplot as plt\n",
    "#             plt.figure(figsize=(10,10))\n",
    "#             plt.imshow(im_crop, cmap = 'gray')\n",
    "#             plt.axis('off')\n",
    "    #         path = 'C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\' + str(label)\n",
    "#             print(im_crop.shape)\n",
    "                cv2.imwrite(file_path, im_crop)\n",
    "#     print(\"done\")\n",
    "    return \n",
    "# video_to_faces('Abhay Singh.mp4', \"Abhay singh\")\n",
    "# video_to_faces('C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Videos\\\\Abhay Singh.mp4', \"Abhay Singh\")\n",
    "# print(max_h, max_w)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692794ae",
   "metadata": {},
   "source": [
    "## Code to convert all the videos to face images and store them in seperate folders according to their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad02862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 9/30 [11:43<29:52, 85.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading frame 690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████▉                              | 19/30 [20:55<13:13, 72.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading frame 740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████▎             | 25/30 [27:32<05:57, 71.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading frame 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [28:58<00:00, 57.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Code to be used later to get a list of sub-folders in a folder\n",
    "## folders = [x[0] for x in os.walk(video_dir)]\n",
    "img_size = 150\n",
    "# Image_Dataset = pd.DataFrame(columns = ['label'] + [f'pixel_{i}' for i in range(img_size*img_size)])\n",
    "videos_list = [f for f in os.listdir(video_dir) if f.endswith(('.mp4'))]\n",
    "for video in tqdm(videos_list):\n",
    "#     print(\"Working on\", video) \n",
    "    video_path = video_dir + \"\\\\\" + str(video)\n",
    "    label = video.replace('.mp4','')\n",
    "    video_to_faces(video_path, label,img_size)\n",
    "#     max_h, max_w = max(h, max_h), max(w, max_w)\n",
    "#     print(max_h, max_w)\n",
    "#     Image_Dataset.to_csv('Image_Dataset.csv')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713f42c",
   "metadata": {},
   "source": [
    "## Generate Random Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d205218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  6.00it/s]\n"
     ]
    }
   ],
   "source": [
    "triplets_data = pd.DataFrame() \n",
    "## add all the created triplets to a Dataframe by flattening them by using df.loc[i] and save it to an excel file\n",
    "def generate_random_triplets(images_path, pairs_per_class):\n",
    "    '''\n",
    "    We will choose the number of triplets based on the number of images in each folder.\n",
    "    Lets say we will choose at max 8 pairs of anchor and positive images from a folder.\n",
    "    If a folder has less than 5 images then we will skip it.\n",
    "    '''\n",
    "    triplets_path = images_path.replace(\"Images\", \"Triplets\")\n",
    "    os.makedirs(triplets_path, exist_ok=True)\n",
    "    anchor = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    folders = [x[0] for x in os.walk(images_path)]\n",
    "    folders = folders[1:]\n",
    "    for folder in tqdm(folders):\n",
    "#         print(folders)\n",
    "        path = os.path.join(images_path, folder)\n",
    "        image_files = [f for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "#         print(\"Image_files\", image_files)\n",
    "        if not image_files:\n",
    "            continue\n",
    "        if len(image_files) < 4:\n",
    "            #Skip folder with less than 8 images\n",
    "            continue\n",
    "        for i in range(pairs_per_class):\n",
    "            current_path = os.path.join(triplets_path, str(f'{folders.index(folder)}_{i}'))\n",
    "            os.makedirs(current_path, exist_ok=True)\n",
    "            ## Choose anchor and positive images\n",
    "            # Choose 2 random numbers to be the indexes for anchor and pos image\n",
    "            anchor_idx, positive_idx = np.random.choice(len(image_files), size=2, replace=False)\n",
    "            anchor_image = cv2.imread(os.path.join(path, image_files[anchor_idx]))\n",
    "            pos_image = cv2.imread(os.path.join(path, image_files[positive_idx]))\n",
    "            # For neg image we will randomly choose a folder and randomly choose an image from it\n",
    "            neg_index = np.random.choice([x for x in range(0, len(folders)) if x != folder]) \n",
    "            neg_folder = os.path.join(images_path, folders[neg_index])\n",
    "            neg_image_files = [f for f in os.listdir(neg_folder) if f.endswith(('.jpg'))]\n",
    "            if not neg_image_files:\n",
    "                continue\n",
    "            neg_image = cv2.imread(os.path.join(neg_folder, np.random.choice(neg_image_files)))\n",
    "            cv2.imwrite(os.path.join(current_path, \"anchor.jpg\"),anchor_image)\n",
    "            cv2.imwrite(os.path.join(current_path, \"pos.jpg\"),pos_image)\n",
    "            cv2.imwrite(os.path.join(current_path, \"neg.jpg\"),neg_image)\n",
    "            anchor.append(anchor_image)\n",
    "            pos.append(pos_image)\n",
    "            neg.append(neg_image)\n",
    "    return anchor, pos, neg\n",
    "image_dir = video_dir.replace(\"Videos\", \"Images\")\n",
    "anchor, pos, neg = generate_random_triplets(image_dir,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3d5d5",
   "metadata": {},
   "source": [
    "## Custom Dataset Class for the triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6088bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, triplet_folder, transform): #transforms.ToTensor()\n",
    "        self.transform = transform\n",
    "        folders = [x[0] for x in os.walk(triplet_folder)]\n",
    "        self.folders = folders[1:]        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.folders)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        folder = self.folders[idx]\n",
    "        label = str(folder)\n",
    "        label = folder.split(\"\\\\\")[-1]\n",
    "        label = label.split(\"_\")[0]\n",
    "        \n",
    "        self.label = label\n",
    "        #Edit the current path\n",
    "        current_path = os.path.join(triplet_folder, str(folder))\n",
    "        image_files = [f for f in os.listdir(current_path) if f.endswith('.jpg')]\n",
    "#         print(\"Folder: \",folder)\n",
    "        anchor = (cv2.imread(os.path.join(current_path, \"anchor.jpg\"))).astype(np.float32)\n",
    "        pos = cv2.imread(os.path.join(current_path, \"pos.jpg\")).astype(np.float32)\n",
    "        neg = cv2.imread(os.path.join(current_path, \"neg.jpg\")).astype(np.float32)\n",
    "#         print(f\"anchor: {anchor}\\npos: {pos}\\nneg: {neg}\")\n",
    "        self.anchor = self.transform(anchor).view(3,150,150) # tensor -> from_numpy\n",
    "        self.pos = self.transform(pos).view(3,150,150)\n",
    "        self.neg = self.transform(neg).view(3,150,150)\n",
    "        return self.label, self.anchor, self.pos, self.neg\n",
    "        \n",
    "triplet_folder = video_dir.replace(\"Videos\", \"Triplets\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "Triplet_Dataset = Dataset(triplet_folder, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005a91f",
   "metadata": {},
   "source": [
    "## Split Dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbfc5f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 150, 150])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.9 * len(Triplet_Dataset))\n",
    "test_size = len(Triplet_Dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(Triplet_Dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=12)\n",
    "test_loader = DataLoader(test_dataset)\n",
    "examples = enumerate(test_loader)\n",
    "i, (l,x,y,z) = next(examples)\n",
    "l[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177bf959",
   "metadata": {},
   "source": [
    "## CNN Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74cad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torchvision import datasets, transforms, models\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # Define a Siamese network with VGG-16 as the base network\n",
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self, vgg16):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         self.vgg16 = vgg16\n",
    "#         self.fc = nn.Linear(1000, 128)  # Assuming VGG-16 output size is 1000, adjust if needed\n",
    "\n",
    "#     def forward_one(self, x):\n",
    "#         x = self.vgg16(x)\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, anchor, positive, negative):\n",
    "# #         anchor = anchor.to(torch.ByteTensor)\n",
    "# #         pos = pos.to(torch.ByteTensor)\n",
    "# #         neg = neg.to(torch.ByteTensor)\n",
    "#         output1 = self.forward_one(anchor)\n",
    "#         output2 = self.forward_one(positive)\n",
    "#         output3 = self.forward_one(negative)\n",
    "#         return output1, output2, output3\n",
    "\n",
    "# # Custom dataset for loading triplets\n",
    "# # Triplet loss function\n",
    "# class TripletLoss(nn.Module):\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(TripletLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def forward(self, anchor, positive, negative):\n",
    "#         distance_positive = torch.norm(anchor - positive, dim=1)\n",
    "#         distance_negative = torch.norm(anchor - negative, dim=1)\n",
    "#         losses = nn.functional.relu(distance_positive - distance_negative + self.margin)\n",
    "#         return losses.mean()\n",
    "\n",
    "# # Load VGG-16 pre-trained on ImageNet\n",
    "# vgg16 = models.vgg16(pretrained=False)\n",
    "# vgg16.features = nn.Sequential(*list(vgg16.features.children())[:-1])  # Remove the last fully connected layer\n",
    "\n",
    "# # Create Siamese network\n",
    "# siamese_net = SiameseNetwork(vgg16)\n",
    "\n",
    "\n",
    "# ## Use GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = 'cpu'\n",
    "# siamese_net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9b574",
   "metadata": {},
   "source": [
    "## Triplet loss function and Main Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c62c3620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (convnet1): Sequential(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(7, 5), stride=(1, 1))\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(5, 3), stride=(1, 1))\n",
       "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (actv): ReLU(inplace=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convnet2): Sequential(\n",
       "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (actv): ReLU(inplace=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convnet3): Sequential(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (actv): ReLU(inplace=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (FCs): Sequential(\n",
       "    (FC1): Linear(in_features=16896, out_features=4224, bias=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (FC2): Linear(in_features=4224, out_features=1024, bias=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (FC4): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = torch.norm(anchor - positive, dim=1)\n",
    "        distance_negative = torch.norm(anchor - negative, dim=1)\n",
    "        losses = nn.functional.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.convnet1 = nn.Sequential(OrderedDict([\n",
    "                            ('conv1', nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(7,5))),\n",
    "                            ('conv2', nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(5,3))),\n",
    "                            ('norm', nn.BatchNorm2d(32)),\n",
    "                            ('actv', nn.ReLU(inplace=True)),\n",
    "                            ('pool', nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        ]))\n",
    "        # ref : https://discuss.pytorch.org/t/is-it-possible-to-specify-a-name-for-each-layer-when-creating-the-model/33637/2\n",
    "\n",
    "        self.convnet2 = nn.Sequential(OrderedDict([\n",
    "                            ('conv1', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)),\n",
    "                            ('conv2', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)),\n",
    "                            ('norm', nn.BatchNorm2d(64)),\n",
    "                            ('actv', nn.ReLU(inplace=True)),\n",
    "                            ('pool', nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        ]))\n",
    "\n",
    "        self.convnet3 = nn.Sequential(OrderedDict([\n",
    "                            ('conv1', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)),\n",
    "                            ('conv2', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3)),\n",
    "                            ('norm', nn.BatchNorm2d(128)),\n",
    "                            ('actv', nn.ReLU(inplace=True)),\n",
    "                            ('pool', nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        ]))\n",
    "        \n",
    "        self.conv     = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3)\n",
    "        \n",
    "        self.FCs      = nn.Sequential(OrderedDict([\n",
    "                            ('FC1', nn.Linear(in_features=16896, out_features=4224)),#3072\n",
    "                            ('relu1', nn.ReLU(inplace=True)),\n",
    "                            ('FC2', nn.Linear(in_features=4224, out_features=1024)),\n",
    "                            ('relu2', nn.ReLU(inplace=True)),\n",
    "                            ('FC3', nn.Linear(in_features=1024, out_features=512)),\n",
    "                            ('relu3', nn.ReLU(inplace=True)),\n",
    "                            ('FC4', nn.Linear(in_features=512, out_features=128)),\n",
    "                            # ('relu3', nn.ReLU(inplace=True)),\n",
    "        ]))\n",
    "        \n",
    "\n",
    "        for m in self.modules():\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('Conv') != -1:\n",
    "                nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')  # He-initialization\n",
    "                # ref : https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_\n",
    "\n",
    "    def semi_forward(self, x):\n",
    "        x = self.convnet1(x)\n",
    "        x = self.convnet2(x)\n",
    "        x = self.convnet3(x)\n",
    "        x = self.conv(x)\n",
    "#         print(x.size(0))\n",
    "#         print(\"Size before view:\", x.size())\n",
    "        if x.size(0) == 1:\n",
    "            x = x.view(1, -1)\n",
    "        else:\n",
    "            x = x.view(-1, 12*11*128)\n",
    "#         print(\"Size after view:\", x.size())\n",
    "#         x = x.view(-1, 6*4*128)\n",
    "#         x = x.view(-1, 17*4*128)\n",
    "#         x = x.view(x.size(1), -1)\n",
    "        # x = x.reshape(-1, 10*6*128)\n",
    "        x = self.FCs(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "#         anchor = anchor.to(torch.ByteTensor)\n",
    "#         pos = pos.to(torch.ByteTensor)\n",
    "#         neg = neg.to(torch.ByteTensor)\n",
    "        output1 = self.semi_forward(anchor)\n",
    "        output2 = self.semi_forward(positive)\n",
    "        output3 = self.semi_forward(negative)\n",
    "        return output1, output2, output3\n",
    "\n",
    "siamese_net = SiameseNetwork()\n",
    "## Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "siamese_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "58125bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(\"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\31Oct.pth\"):\n",
    "#     siamese_net.load_state_dict(torch.load(\"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\31Oct.pth\"))\n",
    "#     siamese_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1f846301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:09<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 10.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 1.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 6.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.9396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 5.2261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 1.0964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 2.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.1457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 7.5571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 26.5741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 3.7266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 10.5760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 46.6249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 8.5409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 2.7364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 7.4499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 10/15 [00:05<00:02,  1.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     14\u001b[0m     _, anchor, pos, neg \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 15\u001b[0m     anchor, pos, neg \u001b[38;5;241m=\u001b[39m \u001b[43manchor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, pos\u001b[38;5;241m.\u001b[39mto(device), neg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m     output1, output2, output3 \u001b[38;5;241m=\u001b[39m siamese_net(anchor, pos, neg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "triplet_loss = TripletLoss()\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)\n",
    "\n",
    "# Load your triplets data into the TripletDataset class\n",
    "# You may need to customize the paths and transformations based on your dataset\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "siamese_net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_loader):\n",
    "        _, anchor, pos, neg = batch\n",
    "        anchor, pos, neg = anchor.to(device), pos.to(device), neg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2, output3 = siamese_net(anchor, pos, neg)\n",
    "        loss = triplet_loss(output1, output2, output3)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ffee4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 3, 150, 150])\n",
      "torch.Size([25, 128])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "siamese_net.eval()\n",
    "encodings = []\n",
    "# Define normalization transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class Identification_Dataset(Dataset):\n",
    "    def __init__(self, transform): #transforms.ToTensor()\n",
    "        self.transform = transform      \n",
    "        identification_dir = video_dir.replace(\"Videos\", \"Identification\")\n",
    "        self.encodings = []\n",
    "        self.representatives = [img for img in os.listdir(identification_dir) if img.endswith('.jpg')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.representatives)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(os.path.join(identification_dir, self.representatives[idx])).astype(np.float32)\n",
    "#         print(image.shape)\n",
    "        self.image = transform(image).view(3,150,150)\n",
    "        return self.image\n",
    "    \n",
    "Identification_Dataset = Identification_Dataset(transform=transform)\n",
    "Identification_loader = DataLoader(Identification_Dataset, batch_size=30)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image in Identification_loader:\n",
    "        image = image.to(device)  # Add batch dimension\n",
    "        print(image.shape)\n",
    "        encoding = siamese_net.semi_forward(image)\n",
    "        encodings.append(encoding)\n",
    "\n",
    "# Convert the list of encodings to a tensor\n",
    "encodings_tensor = torch.cat(encodings, dim=0)\n",
    "\n",
    "print(encodings_tensor.shape)\n",
    "# Size before view: torch.Size([12, 128, 11, 12])\n",
    "# Size after view: torch.Size([66, 3072])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9686bff",
   "metadata": {},
   "source": [
    "## Use the array of encodings created in the earlier Block of code. to calculate the distances between the encoding of a test image and the encoding of the representatives of all the classes. The minimum distance will give the predicted class of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9e7085c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 5 Originally:  ('14',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 8 Originally:  ('17',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 17 Originally:  ('24',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 18 Originally:  ('19',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 21 Originally:  ('21',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 15 Originally:  ('0',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 7 Originally:  ('15',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 19 Originally:  ('4',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 22 Originally:  ('6',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 5 Originally:  ('13',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 0 Originally:  ('0',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 24 Originally:  ('9',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 15 Originally:  ('22',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 10 Originally:  ('6',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 15 Originally:  ('0',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 5 Originally:  ('13',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 18 Originally:  ('7',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 21 Originally:  ('6',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 8 Originally:  ('1',)\n",
      "torch.Size([1, 3, 150, 150])\n",
      "Identified as: 13 Originally:  ('20',)\n",
      "2/20\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    def identify(image, encodings, label):\n",
    "        # Read and normalize the image\n",
    "    #     image = cv2.imread(image_path).astype(np.float32)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        # Forward pass to obtain the encoding\n",
    "        with torch.no_grad():\n",
    "            encoding = siamese_net.semi_forward(image)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = torch.norm(encodings - encoding, dim=1).to('cpu')\n",
    "        distances = distances.numpy()\n",
    "#         print(\"Distances:\", distances)\n",
    "        min_distance_index = np.argmin(distances)\n",
    "        print(\"Identified as:\", min_distance_index, \"Originally: \", label)\n",
    "        return min_distance_index\n",
    "\n",
    "    # Example usage self.anchor = self.transform(anchor).view(3,150,150)\n",
    "    # path = \"C:\\\\Users\\\\dhano\\\\Jupyter Notebooks\\\\Face Recognition\\\\Dataset\\\\Images\\\\Venkatesh\\\\320.jpg\"\n",
    "    count = 0\n",
    "    total = len(test_loader)\n",
    "    for batch in test_loader:\n",
    "        label, anchor, pos, neg = batch\n",
    "    #     anchor = anchor.squeeze().numpy().astype(np.float32)\n",
    "        anchor = anchor.to(device)\n",
    "        print(anchor.shape)\n",
    "        index = identify(anchor, encodings_tensor, label)\n",
    "        if index == int(label[0]):\n",
    "            count = count + 1\n",
    "    print(f\"{count}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ab215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c6e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6efabb06",
   "metadata": {},
   "source": [
    "# Code to Open Webcam, draw a rectangle around face and label it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "34a7e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access video from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "def detect_bounding_box(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    x, y, w, h = 0,0,0,0\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        x, y, w, h = x, y, w, h\n",
    "    return faces, x, y\n",
    "while True:\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    faces, x, y = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "    title = 'Face'\n",
    "    cv2.putText(video_frame,title , (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (30,255,30), 2)\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )\n",
    "    # display the processed frame in a window named \"My Face Detection Project\"\n",
    "    # add label to the face\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac866ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
